# -*- coding: utf-8 -*-
"""frequency analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJwQDCHjSQF0B2cN9z7T7QNLSLmwTS74

## Task 1: Import data

Import data from NLTK (see http://www.nltk.org/book_1ed/ch02.html), for example, using the Gutenberg dataset:
"""

import nltk
from nltk.corpus import gutenberg
nltk.download('gutenberg')
for fileid in gutenberg.fileids():
    print(fileid, gutenberg.raw(fileid)[:65])

"""## Task 2: Use NLTK's FreqDist functionality

Use the `FreqDist` functionality as shown in https://www.nltk.org/book/ch01.html and http://www.nltk.org/book_1ed/ch02.html. 

For the datasets available via NLTK you can either apply tokenization with `word_tokenize` or rely on the `.word` functionality, which provides you with tokenized output:
"""

fdist1 = nltk.FreqDist(gutenberg.words("austen-emma.txt"))
# Print out most frequent 50 words with their counts. 
# Hint: you need to use most_common(number_of_words) method applied to fdist1
fdist1.most_common(50)

"""What can you tell about the most frequent words in this text?

Let's try visualising cumulative frequency of the most frequent $30$ words:
"""

# Hint: you need to use plot(number_of_words, cumulative=True) method applied to fdist1
fdist1.plot(30,cumulative=True)

"""What does this plot suggest?

## Task 3: Implement FreqDist from scratch

Collect words, calculate their frequency, and return a dictionary sorted in the reverse order:
"""

import operator

def collect_word_map(word_list):
    word_map = {}
    for a_word in word_list:
        word_map[a_word] = word_map.get(a_word,0) +1
                            # update the count for a_word in word_map by 1. 
                           # Hint: word_map.get(a_word) returns the current count,
                           #       word_map.get(a_word, 0) allows you to cover cases where current word count is 0 
    return word_map
    
# Let's sort the word frequency map by word counts, 
# starting from the largest count (reverse order), 
# and print up to 10 most frequent words
word_map = collect_word_map(gutenberg.words("austen-emma.txt"))
sorted_map = (sorted(word_map.items(), key=operator.itemgetter(1)))[::-1]
print(sorted_map[:10])

"""Let's calculate the percentage of the content covered by specific (most frequent) words. E.g., what percentage of words used in text are commas?"""

def collect_percentage_map(word_map, up_to):
    total_count = sum(word_map.values())
    sorted_map = (sorted(word_map.items(), key=operator.itemgetter(1)))[::-1] #sort the word frequency map by word counts, starting from the largest count (reverse order)
    percentage_map = [(item[0], 100*float(item[1])/float(total_count)) for item in sorted_map[:up_to]]
    return percentage_map

print(collect_percentage_map(word_map, 50))

"""Finally, let's visualise the cumulative frequency counts as a histogram:"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
# %matplotlib inline

def visualise_dist(word_map, up_to):
    width = 10.0
    percentage_map = collect_percentage_map(word_map, up_to)# apply collect_percentage_map from above
    sort_map = {}
    rank = 0
    cum_sum = 0
    # Store cumulative percetage of coverage
    for item in percentage_map:
        rank += 1
        cum_sum += item[1]
        sort_map[rank] = cum_sum
    # How much do the top n words account for?
    print("Total cumulative coverage = %.2f" % cum_sum + "%")
    
    fig, ax = plt.subplots()
    plt.title("Cumulative coverage of the top " + str(up_to) + " words")
    plt.ylabel("Percentage")
    plt.xlabel("Top " + str(up_to) + " words")
    # Build the histogram for the percentages
    plt.bar(range(len(sort_map)), sort_map.values())
    # Label x axis with the ranks of the 1st to n-th most frequent word
    # printing out each 5-th label on the axis
    start, end = ax.get_xlim()
    ax.xaxis.set_ticks(np.arange(start, end+1, 5))
    plt.show()
    
# Explore statistics with a different number of top n words
visualise_dist(word_map, 50)

"""What does this cumulative distribution suggest?

## Task 4: Apply to other texts

This is an open-ended task.
"""

for fileid in gutenberg.fileids():
  print(f'-------------For word in file {fileid}-----------')
  fdist1 = nltk.FreqDist(gutenberg.words(fileid))
  fdist1.most_common(50)
  fdist1.plot(30,cumulative=True)
  word_map = collect_word_map(gutenberg.words(fileid))
  print(collect_percentage_map(word_map, 50))
  visualise_dist(word_map, 50)
  print('\n\n\n')