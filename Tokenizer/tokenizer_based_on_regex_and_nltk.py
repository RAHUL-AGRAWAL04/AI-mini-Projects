# -*- coding: utf-8 -*-
"""tokenizer based on regex and NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VkxVWKwRFEtVYq5UtpJEUa8nXFRMXnVw

- Implement a simple word tokenization algorithm based on regular expressions.
- Use one of the available tokenizers from NLTK and compare the results.

## Task 1: Import data

Import data from NLTK (see http://www.nltk.org/book_1ed/ch02.html):
"""

import nltk
from nltk.corpus import webtext
nltk.download('webtext')
print(webtext.raw("pirates.txt"))

"""Extract the first sentence as an example:"""

sentence = webtext.raw("pirates.txt").split('\n')[0]
           #split it by newline \n, and extract the first sentence (i.e., at position 0)
print(sentence)

"""## Task 2: Implement a simple tokenization algorithm

Split the sentence by whitespace:
"""

print(sentence.split())

"""Do you see any problems with this approach?

Let's try pattern-matching:
"""

import re

tokens = re.split(r"([-:' ])+" ,sentence)

tokenized = [x for x in tokens if x != '' and x not in '- \t\n.,;:!?[]']
print(tokenized)

"""Does this solve all the problems?

## Task 3: Apply an out-of-the-box algorithm

Use NLTK's tokenizer (see https://www.nltk.org/book/ch03.html):
"""

from nltk import word_tokenize
nltk.download('punkt')
tokens = word_tokenize(sentence)
print(tokens)

"""Let's convert all words to lower case:"""

words = [w.lower() for w in tokens]
print(words)

"""Finally, sort the words in alphabetical order:"""

vocab = sorted(set(words))
print(vocab)

"""## Task 4: Apply to other sentences

This is an open-ended task.
"""

sentences = webtext.raw("pirates.txt").split('\n')
vocab = []
for sentence in sentences:
  tokens = word_tokenize(sentence)
  #print(tokens)
  words = [w.lower() for w in tokens]
  #print(words)
  vocab += set(words)
vocab = sorted(set(vocab))
print(vocab)